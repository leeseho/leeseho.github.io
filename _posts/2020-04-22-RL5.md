---
layout: post
title: "[강화학습-5] Value function approximation"
categories : dev
tags : ai rl cs dev
comments : true
use_math: true
---




# Value function Approximation
 - State나 action의 개수가 매우 많거나, 실수처럼 Continuous 한 상황에서는 memory에 다 담을 수도 없을 뿐아니라 계산이 거의 불가능하다. 그래서 이러한 상황에서 적용하기 위한 방법이 Approximation이다.  
 - V hat, Q hat (W를 추가로 인자로 가짐)등 approximator를 이용해 실제 true V, true Q를 학습하는 것이다.  
 - 처음 본 모델에 대해서도 어느정도 Genralized 된 답을 내준다는 특징이 있다.

## Function Approximator의 종류?
 - Linear combinations of features, Neural network, Decison tree, Nearest neighbor ...  etc  
 - 이 중 Differentialble 한 것은 `Linear combinations of features`와 `Neural network` 이다.  


## Incremental method  
### Gradient Descent
  인공지능 기초... skip


### Linear Value Function Approximator
 feature vector * W의 합(내적의 형태)으로 나타내지는 함수. (선형이라고 가정)  
![feature](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-22-21-06-39.png?raw=true)  
 Value의 Update :  
 ![Update1](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-22-21-08-33.png?raw=true)  


 사실 Linear Funciton Approximator의 특별한 경우가 Table look up방법이다. Feature가 모두 1인 경우가 table look up과 동일하기 때문이다. 

저 true value function 자리에 지금까지 배웠던, 다양한 target 함수를 넣으면 된다. 
![IPA](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-22-21-17-27.png?raw=true)  

![Action VF](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-22-21-20-38.png?raw=true)  




## Batch method
 Agent가 이전에 쌓은 경험을 반복해서 이용하는 방법. 한번 update하고 버리는 Incremental method와 달리 `Experiment replay`를 한다. Experiment replay는 앞서 쌓은 데이터를 재사용하는 전략이다.  


 ### DQN  
 보통 off-policy + non linear 함수, TD 방법이면 수렴성이 보장이 안된다. DQN은 experience replay와 fixed Q-targets을 이용해 해결하였다. fixed Q-target은 Q를 두 개(계속 업데이트 하는 Q, 고정하는 Q)를 활용하는 전략이다.  

<br><br><br>  


# Policy Gradient
 이전까지 (State/Action) Value function을 parameter를 이용해서 근사시키는 것을 했다면, 지금부터는 Policy 자체를 파라메터화한다. 예를들면 Policy를 Neural net 등의 함수를 이용해 나타내는 것이다.  
![prl](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-27-11-51-12.png?raw=true)  
![prl_2](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-27-11-51-39.png?raw=true)  

 그러나 Pi에 대한 Gradient를 구하는 것이 아니라, J에 대한 Gradient를 구하는 것임을 혼동하면 안된다. J를 Policy objective function(목적함수)이라고 하는데 아래 세 개의 식 중, 아무거나 써도된다고 한다.  

## Objective function
![J_function](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-27-10-44-02.png?raw=true)  
![d_pi](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-27-12-07-36.png?raw=true)
![..](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-27-10-50-14.png?raw=true)


## Finite Difference 방법
 Policy gradient를 구하기위한 가장 단순한 방법으로, theta(1~n)를 아주 조금(epsilon) 변경해본 후 편미분해보는 방법이다. 그러나 이 방법은 한 번 update시 n번의 계산이 필요하다는 점에서 비효율적이다. 그래서 거의 사용하지 않는다고 한다.  
 ![Finite Difference](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-27-10-54-41.png?raw=true)  

## Socre Function
 ![Likelihood ratio trick](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-27-12-09-06.png?raw=true)  


## One Step MDP
![OSMDP](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-27-12-14-51.png?raw=true)  
 위의 likelihood ratio 트릭을 이용하면 위와같은 식을 얻을 수 있다.  

## Monte-Carlo Policy Gradient  
![mcpg](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-27-11-31-44.png?raw=true)  
 위 알고리즘은 알파고에서 사용된 알고리즘이다. theta는 프로기사의 기보로 초기화하였으며, 직접 두면서 위 과정으로 update하였다고 한다.  

 ...