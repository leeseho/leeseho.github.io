---
layout: post
title: "Tensorflow 연습 1-1"
subtitle: "Customization : Tensors and Operations"
categories : [ML]
date: 2020-1-3 17:00:00 -0900
background: '/img/posts/06.jpg'
---


## 텐서와 넘파이 배열간 상호 변환
 텐서에 대한 넘파이 연산은 텐서를 자동으로 넘파이 배열로 변환한다.  
 또한 그 반대로 넘파이 배열에 대한 텐서연산은 자동으로 텐서로 변환시켜준다.

가장 간단한 예로, tensor는 numpy() 메소드로 넘파이 배열로 변환가능하다.
``` python
tensor = tf.constant([1, 2, 3], dtype = tf.float32)
print(tensor.numpy())
```


## GPU 가속
 텐서플로우는 어떠한 명시적인 코드를 작성하지 않아도 알아서 CPU와 GPU에 알아서 연산을 할당한다고 한다. 그리고 직접 CPU 또는 GPU에 강제로 연산을 할당할 수도 있다.  
 `tf.test.is_gpu_available()` 메소드를 이용해서 GPU 연산이 가능한지 알 수 있으며,  
 `tensor.device.endswith("GPU:0")`를 통해 해당 텐서가 0번쨰 GPU 메모리에 올라가있는지 확인할 수 있다. `tensor.device`는 텐서를 처리하는 호스트장치의 풀네임을 제공한다고 한다. 텐서가 호스트의 N번째 GPU에 놓여지면 문자열은 GPU:<N>으로 끝납니다. CPU인 경우 CPU:<N>이다.

 그리고, `tf.device`를 이용해 명시적으로 장치를 배치할 수 있다. 예를 들면 다음과 같이 활용할 수 있다.

 ``` python
 if tf.test.is_gpu_available(): # GPU가 사용가능하다면,
  print("On GPU:")
  with tf.device("GPU:0"): # GPU:0을 이용하여 연산강제. Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc...
    x = tf.random.uniform([1000, 1000])
    assert x.device.endswith("GPU:0") # GPU:0
    time_matmul(x)
 ```

 ## Dataset
  Tensorflow Guide에는 dataset에 대해
 > 모델에 데이터를 제공하기 위한 파이프라인을 구축하기 위해 tf.data.Dataset API를 사용..  tf.data.Dataset API는 모델을 훈련시키고 평가 루프를 제공할, 간단하고 재사용 가능한 모듈로부터 복잡한 입력 파이프라인을 구축하기 위해 사용됩니다..

 라고한다.
이전에 Linear Regression할때 사용한적이 있는데,  
``` python
#
dataset = tf.data.Dataset.from_tensor_slices(( X , Y )) 
dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size ) 
iterator = dataset.__iter__()  # dataset에 대한 반복자

# ... 중략

for i in range(num_epochs) :
    epoch_loss = list()
    for b in range(int(num_samples/batch_size)): 
        x_batch, y_batch = iterator.get_next() # 다음 batch 선택

#... 생략
```

 데이터에서 batch size를 지정하고, 셔플하는 등의 기능을 수행하고,
 한 dataset에서 batch를 선택(for문 내)하는 반복자도 제공한다.


 ```python
 ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])
 ds_tensors = ds_tensors.map(tf.square).shuffle(2).batch(2)
 #map(tf.square)은 텐서의 원소에 대해 제곱 기능 수행
 #shuffle : 셔플 수행
 #batch : batch size 지정

for x in ds_tensors: # 반복가능
  print(x)
 ```