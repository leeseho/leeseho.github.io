---
layout: post
title: "[강화학습-2] Bellman Equation"
tags : AI CS Dev
comments : true
use_math: true
---



# Bellman Equation 
 Bellman equation은 현재 상태의 Value function을 다음 상태의 Value function과 연관지은 식이다. 이것을 간단하게 식으로 표현하면...   

 ![eq0_bellman_equation](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-13-22-14-03.png?raw=true)  

 Bellman equation은 Expectation과 Optimality 두가지 종류가 있다.
 

 ## State value function을 Action과 연관짓기
 MDP에서 agent는 return이 높은 방향으로 state를 변경해가야한다. 

 그런데 실생활에서 저 State를 잘 생각해보면, 우리(agent)가 어떤 action을 취하느냐에 따라, 어떤 또다른 State로 바뀌는 것이 맞다.

  따라서 State value function(V(S))를 action value function(q(s,a))와 각 action에 대한 확률인 Policy의 곱으로 나타낼 수 있다.

  즉, Policy * q(S, A) 형태이다. `각 action value에 대해, 합이 1인 확률을 각각 곱해서 더하므로` 기대값(Expectation)의 형태이다.  

 ![eq1_V(s)_q(s)](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-13-21-42-20.png?raw=true)
 
 
 ![eq2_q-function](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-13-21-57-41.png?raw=true)

  위 식에서도 state value function을 없애기 위해, 두 식을 합치면...  

  ![eq3_Bellman_q-function](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-13-22-00-23.png?raw=true)  

 위와같이 나타낼 수 있다. 위 식이 action value function으로 나타낸 Bellman equation이다.  