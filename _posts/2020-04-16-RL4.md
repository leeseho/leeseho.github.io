---
layout: post
title: "[강화학습-4] Monte Calro, Temporal Difference"
tags : AI RL CS Dev
comments : true
use_math: true
---


# Monte Carlro, Temporal Difference ?

MC와 TD의 가장 차이점은 MC의 경우 일련의 완전한 시나리오로부터 학습을 한다는 점이다. 예를들면, 3x3행렬의 좌상단 끝에서 우하단 끝으로 이동하는 것이 목표라고 할 때, 어떤 경로로든 오른쪽 아래 끝도착한 뒤에야 한번의 학습이 이루어지는 것이 MC이고, TD는 끝까지 도달하지 않고도 `현재 위치보다는 가까운 다음 지점의 정보로부터` 학습하는 방식이다. 유튜브 강의에서 어떤분이 비유적으로 내일의 날씨를 예측하는데 오늘 예측한 것보다는 내일 예측한 정보가 더 정확하다라고 표현하신 것이 적절한 것 같다.  


# Sample backup
 이전의 DP 방법의 경우, 모든 경우에 대한 value값을 계산하였다. 이를 Full Width backup이라고도 한다. 그러나 Learning의 방법에서는 그 중 일부를 시도해보고, 일부로 학습하는 Sample back up의 방식을 적용한다.


# Monte Carlro Method(MC) 
 


# Temporal Difference learning  (Q-Learning or TD)  

