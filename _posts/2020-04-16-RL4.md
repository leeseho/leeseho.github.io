---
layout: post
title: "[강화학습-4] Monte Calro, Temporal Difference.. Q-learning"
categories : dev
tags : ai rl cs dev
comments : true
use_math: true
---


# Prediction, Control
 현재의 Policy를 이용해서 주변을 탐색(경험)하면서 value function을 구하는 것이 `Prediction` 과정이며,
 Prediction을 통해 알게된 정보를 이용해 Policy를 좀 더 정확하게 하는 과정을 `Control`이라고 한다.  


# Monte Carlro, Temporal Difference ?

MC와 TD의 가장 차이점은 MC의 경우 일련의 완전한 시나리오로부터 학습을 한다는 점이다. 예를들면, 3x3행렬의 좌상단 끝에서 우하단 끝으로 이동하는 것이 목표라고 할 때, 어떤 경로로든 오른쪽 아래 끝도착한 뒤에야 한번의 학습이 이루어지는 것이 MC이고, TD는 끝까지 도달하지 않고도 `현재 위치보다는 가까운 다음 지점의 정보로부터` 학습하는 방식이다. 유튜브 강의에서 어떤분이 비유적으로 내일의 날씨를 예측하는데 오늘 예측한 것보다는 내일 예측한 정보가 더 정확하다라고 표현하신 것이 적절한 것 같다.  


# Sample backup
 이전의 DP 방법의 경우, 모든 경우에 대한 value값을 계산하였다. 이를 Full Width backup이라고도 한다. 그러나 Learning의 방법에서는 그 중 일부를 시도해보고, 일부로 학습하는 Sample back up의 방식을 적용한다.


# Monte Carlro Method(MC) 
 MC는 하나의 시나리오를 끝까지 가보고, 그곳에서 얻은 정답으로 지금까지 지나온 STate에서의 value를 업데이트한다. 이 때 value는 Q(Action value finction)이다. Q는 model을 몰라도 찾을 수 있지만, V는 아니기 떄문.    


# Temporal Difference learning  (Q-Learning or TD)  
 TD에서 업데이트에 이용하는 값은 실제값이 아니라 예측이이다.  


# e(epsilon) greedy Exploration
  Prediction은 Q를 evaluation하는 것으로 정하고, Greedy하게 Policy를 업데이트한다면, 모든 상태를 다 시도해본다는 것을 보장할 수 없을 것이다. 그래서 이를 해결하기 위한 간단한 아이디어가 `e-greedy`이다. 간단히 `e` 확률로는 임의의 action을 취하게 함으로써 모든 state를 exploration하는 것을 보장할 수 있다. 

# GLIE MC Control
 MC + e-greedy 방법을 취할 때, 수렴하기 위한 조건은 모든 상태-액션 쌍이 무한에 가깝게 explored 되어야하고, policy는 결국 greedy poloicy에 수렴해야한다.  이를 만족하는 하나의 예가 아래와 같은 GLIE MC control이다.  
 N <- N + 1   
 Q <- Q + 1/N (G - Q)  




# S.A.R.S'.A'  
 SARSA는 다음 상태의 Q(S',A')에서 현재의 예측값(Q(S,A))을 뺀 것을 오차로 하는 방법이다.  
![SARSA](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-20-11-01-39.png?raw=true)   
SARSA는 기본적으로 1 step마다 업데이트를 하는 것이 기본이나, n-step후에 학습하는 것도 역시 가능하다. 이것을 `n-step Sarsa`라고한다. 수식에서는 간단하게 기존 TD Target(R+Q(S',A'))을 n step만큼으로 변경될 뿐이다.  
 

# Off-Policy Learning  
$\pi$ (a|s)와 별개로 Action을 sampling하는 behaviour policy u(a|s)를 따로 가진다.  
즉 다른 
off-policy learning 방법은 importance sampling과 q learning 두가지가 존재한다.


## Importance Sampling
![importance sampling](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-21-11-11-03.png?raw=true)  
이 수식의 의미는 어떤 확률 분포를 따르는 모델을 이용해서, 또 다른 확률 분포를 따르는 모델의 기댓값을 구할 수 있다는 것이다. 


# Q learning
  ![Q_1](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-21-11-41-33.png?raw=true)  

  ![Q_2](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-21-11-46-56.png?raw=true)

  현재 상태에서 선택할 Action은 behaviour policy(u)를 따라 선택하고, update 할 때, 다음 State의 action은 $$\pi$$를 따른다.
  ... 

