---
layout: post
title: "[강화학습-3] Dynamic Programming/ MC / TD "
tags : AI RL CS Dev
comments : true
use_math: true
---


# Bellman equation을 이용해 MDP를 푸는 방법
 Bellamn equation을 이용해 MDP를 푸는 방법은 3가지가 있다. DP(Dynamic Programming), Monte-carlo Learning, Temporal Difference Learning 세 가지 이다. 이중 DP는 model(return과 state transition probability)를 알고 있다고 가정하고 이를 푸는 방법이다.
 DP가 아닌 monte-carlo나 Temporal difference leanring은 저 model을 알 수 없기 때문에, 직접 경험을 통해서만 학습이 가능하다는 차이점이 있다.


## Planning vs Learning  
 DP와 같이 model(`reward, state transition probability`)에 대한 정보를 모두 알고 있는 상황, 모든 경우의 수에 대해 MDP를 푸는 것을 Planning이라고 하며, Learning은 현실적으로 Planning이 불가능하므로, 샘플을 추출하고 그것에 대해 학습하는 것이다.



# DP  
## Policy Iteration
 Dynamic Programing을 적용한 예시가 Poicy iteration임.  

### Prediction과 Control의 반복  
Prediction은 policy를 따라 value function을 구하는 것이고(`Policy Evaluation`),  
Control은 policy를 발전(`Policy Imrpovement`)시키는 것이다. 그리고 이 과정을 반복(k번)하면, 점점 ture value function에 가까워질 것이다라는 것이 아이디어이다.  

![evaluatiom](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-15-18-25-38.png?raw=true)  

 위 식의 계산을 반복하면, Value function이 True value function에 가까워진다.  

 그 후, 랜덤 uniform 이었던 policy를 위에서 구한 해당 state에서의 최대한의 value를 얻을 수 있는 policy로 업데이트하면 된다.  

 ![improvement](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-15-18-29-26.png?raw=true)  



 그러나 실제 상황에서는 어떤 행동을 했을 때 얼만큼의 reward를 받는지, 어떤 상태가 되는 것이 유리한지 전혀 모르는 경우가 많다.  
 그래서 이를 위해서 Monte Carlo(MC), Temporal Difference(TD) 등의 Learning 기법이 등장하게 된 것이다.  


# Monte Carlro, Temporal Difference ?

MC와 TD의 가장 차이점은 MC의 경우 일련의 완전한 시나리오로부터 학습을 한다는 점이다. 예를들면, 3x3행렬의 좌상단 끝에서 우하단 끝으로 이동하는 것이 목표라고 할 때, 어떤 경로로든 오른쪽 아래 끝도착한 뒤에야 한번의 학습이 이루어지는 것이 MC이고, TD는 끝까지 도달하지 않고도 `현재 위치보다는 가까운 다음 지점의 정보로부터` 학습하는 방식이다. 유튜브 강의에서 어떤분이 비유적으로 내일의 날씨를 예측하는데 오늘 예측한 것보다는 내일 예측한 정보가 더 정확하다라고 표현하신 것이 적절한 것 같다.  


# Monte Carlro Method(MC) 
 


# Temporal Difference learning  (Q-Learning or TD)  

