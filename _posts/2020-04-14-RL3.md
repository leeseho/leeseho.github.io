---
layout: post
title: "[강화학습-3] Dynamic Programming"
categories : dev
tags : ai RL CS Dev
comments : true
use_math: true
---


# Bellman equation을 이용해 MDP를 푸는 방법
 Bellamn equation을 이용해 MDP를 푸는 방법은 3가지가 있다. DP(Dynamic Programming), Monte-carlo Learning, Temporal Difference Learning 세 가지 이다. 이중 DP는 model(return과 state transition probability)를 알고 있다고 가정하고 이를 푸는 방법이다.
 DP가 아닌 monte-carlo나 Temporal difference leanring은 저 model을 알 수 없기 때문에, 직접 경험을 통해서만 학습이 가능하다는 차이점이 있다.


## Planning vs Learning  
 DP와 같이 model(`reward, state transition probability`)에 대한 정보를 모두 알고 있는 상황, 모든 경우의 수에 대해 MDP를 푸는 것을 Planning이라고 하며, Learning은 현실적으로 Planning이 불가능하므로, 샘플을 추출하고 그것에 대해 학습하는 것이다.



# DP  
## Policy Iteration
 Dynamic Programing을 적용한 예시가 Poicy iteration임.  

### Prediction과 Control의 반복  
`Prediction`은 policy를 따라 value function을 구하는 것이고(`Policy Evaluation`),  
`Control`은 policy를 발전(`Policy Imrpovement`)시키는 것이다. 그리고 이 과정을 반복(k번)하면, 점점 ture value function에 가까워질 것이다라는 것이 아이디어이다.  

![evaluatiom](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-15-18-25-38.png?raw=true)  

 위 식의 계산을 반복하면, Value function이 True value function에 가까워진다.  

 그 후, 랜덤 uniform 이었던 policy를 위에서 구한 해당 state에서의 최대한의 value를 얻을 수 있는 policy로 업데이트하면 된다.  

 ![improvement](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-15-18-29-26.png?raw=true)  


## 한계점  
 그러나 실제 상황에서는 어떤 행동을 했을 때 얼만큼의 reward를 받는지, 어떤 상태가 되는 것이 유리한지 전혀 모르는 경우가 많다.  
 그래서 이를 위해서 Monte Carlo(MC), Temporal Difference(TD) 등의 Learning 기법이 등장하게 된 것이다.  

