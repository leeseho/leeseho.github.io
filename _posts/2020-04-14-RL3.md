---
layout: post
title: "[강화학습-3] Dynamic Programming "
tags : AI RL CS Dev
comments : true
use_math: true
---


# Bellman equation을 이용해 MDP를 푸는 방법
 Bellamn equation을 이용해 MDP를 푸는 방법은 3가지가 있다. DP(Dynamic Programming), Monte-carlo Learning, Temporal Difference Learning 세 가지 이다. 이중 DP는 model(return과 state transition probability)를 알고 있다고 가정하고 이를 푸는 방법이다.
 DP가 아닌 monte-carlo나 Temporal difference leanring은 저 model을 알 수 없기 때문에, 직접 경험을 통해서만 학습이 가능하다는 차이점이 있다.


## Planning vs Learning  
 DP와 같이 model에 대한 정보를 모두 알고 있는 상황, 모든 경우의 수에 대해 MDP를 푸는 것을 Planning이라고 하며, Learning은 현실적으로 Planning이 불가능하므로, 샘플을 추출하고 그것에 대해 학습하는 것이다.



# DP  
## Policy Iteration
 Dynamic Programing을 적용한 예시가 Poicy iteration임.  

### Prediction과 Control의 반복  
Prediction은 policy를 따라 value function을 구하는 것이고(`Policy Evaluation`),  
Contorl은 policy를 발전(`Policy Imrpovement`)시키는 것이다. 그리고 이 과정을 반복(k번)하면, 점점 ture value function에 가까워질 것이다라는 것이 아이디어이다.  
![eval_improv](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-15-13-44-15.png?raw=true)

 여기서도 볼 수 있듯이, 


# Monte Carlro Method  


# Temporal Difference learning  (Q-Learning)  

