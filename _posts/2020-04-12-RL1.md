---
layout: post
title: "[강화학습] Markove Decision Process"
tags : AI
comments : true
use_math: true
---

# 강화 학습(Reinforcement Learning)
 강화학습은 사람이 잘못하면 벌을 받고, 잘하면 상을 받는 것과 같은 방식으로 학습하는 방법이다. 

# MDP(Markov Decision Process)  
## Markov Process  
 먼저 Markov Proces란 현재의 상태(state)가 이전의 상태에만 영향을 받는 이산적인 확률 과정이다. Markov Process에서 어떤 사건은 바로 이전의 사건에만 영향을 받으며, 이 이전에 어떤 과정을 거쳤든 상관없다. 이러한 성질을 `Markov Property`라고 하며, `Memoryless`하다고도 한다. 이를 수식으로 나타내면 아래와 같다.  
![Markov Property](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-13-00-15-39.png?raw=true)  
  
## Return  
 상태가 변하면서 얻는 return을 생각해볼 수 있는데, 수식으로는 다음과 같이 나타낸다. 특정 시간의 return은 R로 R의 discount factor가 곱해진 return의 합은 G로 나타낸다.
 ![return](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-13-01-10-29.png?raw=true)  


## Policy  
 어떤 State에서 각 Action을 취할 확률을 Policy라고 한다. 최적의 Policy를 찾는다는 것은 Agent가 특정 상황에서 어떤 Action을 취했을 때 Return이 최대가 될지를 학습한다는 의미이다. 즉, 강화학습의 목표이다. 수식으로 표현하면 다음과 같다.  
![Policy](https://github.com/leeseho/leeseho.github.io/blob/master/_posts/images/2020-04-13-00-35-20.png?raw=true)  


## MDP에서의 Value : V(S)  
 value function이란 return 값의 `기대치(E)`이다. state에 대한 value function과 action에 대한 value function이 있다.

 어떤 Agent가 Action value function에 따라 모든 일련의 시나리오를 따라 상태가 변화하였을 때 얻을 수 있는 return의 `기대치(E)`를 그 상태의 Value function이라고 한다.

 




 ## 참고 자료 
 https://sumniya.tistory.com/3?category=781573  
 https://dnddnjs.gitbooks.io/rl/content/mdp.html
 